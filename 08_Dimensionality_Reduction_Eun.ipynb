{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Seminar\n",
    "### Chapter08. Dimensionality Reduction\n",
    "<br>\n",
    "19.09.19  \n",
    "\n",
    "\n",
    "JaeEun Yoo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Many Machine Learning problems involve thousands or even millions of features for each training instance. \n",
    "    * Not only does this make training extremely slow, it can also make it much harder to find a good solution, as we will see. \n",
    "    * This problem is often referred to as the **curse of dimensionality.**\n",
    "* Fortunately, in real-world problems, it is often possible **to reduce the number of features considerably**, turning an intractable problem into a tractable one. \n",
    "    * For example, consider the MNIST images (introduced in Chapter 3): \n",
    "    * the pixels on the image borders are almost always white, so you could completely drop these pixels from the training set without losing much information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/08_01.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Figure 7-6 confirms that these pixels are utterly unimportant for the classification task. \n",
    "* Moreover, two neighboring pixels are often highly correlated: \n",
    "    * if you merge them into a single pixel (e.g., by taking the mean of the two pixel intensities), you will not lose much information.\n",
    "* Apart from speeding up training, dimensionality reduction is also extremely useful for **data visualization (or DataViz)**. \n",
    "* Reducing the number of dimensions down to two (or three) makes it possible to **plot a high-dimensional training set on a graph** and often gain some important insights by visually detecting patterns, such as **clusters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this chapter we will discuss the curse of dimensionality and get a sense of what goes on in highdimensional space. \n",
    "* Then, we will present the two main approaches to dimensionality reduction (projection and Manifold Learning), and we will go through three of the most popular dimensionality reduction techniques: <u>PCA, Kernel PCA, and LLE.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **WARNING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reducing dimensionality does lose some information (just like compressing an image to JPEG can degrade its quality), so even though it will **speed up** training, it may also make **your system perform slightly worse.** \n",
    "* It also makes your <u>pipelines a bit more complex</u> and thus <u>harder to maintain.</u> \n",
    "* So you should first **try to train your system with the original data before considering using dimensionality reduction** if training is too slow. \n",
    "    * In some cases, however, reducing the dimensionality of the training data may **filter out some noise and unnecessary details** and thus result in higher performance (but in general it won’t; it will just speed up training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are so used to living in three dimensions that our intuition fails us when we try to imagine a highdimensional space. \n",
    "    * Even a basic 4D hypercube is incredibly hard to picture in our mind (see Figure 8-1), let alone a 200-dimensional ellipsoid bent in a 1,000-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/08_02.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It turns out that many things behave very differently in high-dimensional space. \n",
    "    * For example, if you pick a random point in a **unit square (a 1 × 1 square)**, it will have only about a <u>0.4% chance of being located less than 0.001 from a border</u> (in other words, it is very unlikely that a random point will be “extreme” along any dimension). \n",
    "    * But in a **10,000-dimensional unit hypercube** (a 1 × 1 × ⋯ × 1 cube, \n",
    "    with ten thousand 1s), this probability is greater than <u>99.999999%.</u> \n",
    "* Most points in a high-dimensional hypercube are very close to the border."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here is a more troublesome difference: \n",
    "    * if you pick two points randomly in a unit square, the distance between these two points will be, on average, roughly **0.52**. \n",
    "    * If you pick two random points in a unit 3D cube, the average distance will be roughly **0.66**. \n",
    "    * But what about two points picked randomly in a 1,000,000-dimensional hypercube? \n",
    "* Well, the average distance, believe it or not, will be about **408.25**(roughly ![Figure 6-2](./img/08_03.PNG)! \n",
    "* This is quite counterintuitive: how can two points be so far apart when they both lie within the same unit hypercube? \n",
    "    * **This fact implies that high-dimensional datasets are at risk of being very sparse:**\n",
    "    * most training instances are likely to be far away from each other. \n",
    "    * Of course, this also means that a new instance will likely be far away from any training instance, **making predictions much less reliable than in lower dimensions,** since they will be based on much larger extrapolations. \n",
    "    * extrapolations : 보외법, 원래의 관찰 범위를 넘어서서 다른 변수와의 관게에 기초하여 변수의 값을 추정하는 과정.\n",
    "* In short, **the more dimensions the training set has, the greater the risk of overfitting it.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In theory, one solution to the curse of dimensionality could be to **increase the size of the training set to reach a sufficient density of training instances.** \n",
    "* Unfortunately, in practice, the number of training instances required to reach a given density grows exponentially with the number of dimensions. \n",
    "* With just 100 features (much less than in the MNIST problem), you would need more training instances than atoms in the observable universe in order for training instances to be within 0.1 of each other on average, assuming they were spread out uniformly across all dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Approaches for Dimensionality Reduction  \n",
    "Before we dive into specific dimensionality reduction algorithms, let’s take a look at the <u>two main approaches to reducing dimensionality</u>: **projection and Manifold Learning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In most real-world problems, training instances are not spread out uniformly across all dimensions.\n",
    "* Many features are almost constant, while others are highly correlated (as discussed earlier for MNIST). \n",
    "* As a result, **all training instances actually lie within (or close to) a much lower-dimensional subspace of the high-dimensional space.**  \n",
    "* This sounds very abstract, so let’s look at an example. In Figure 8-2 you can see a 3D dataset represented by the circles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/08_04.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notice that all training instances lie close to a plane: this is a lower-dimensional (2D) subspace of the high-dimensional (3D) space. \n",
    "* Now if we project every training instance perpendicularly onto this subspace (as represented by the short lines connecting the instances to the plane), we get the new 2D dataset shown in Figure 8-3. Ta-da! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/08_05.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have just reduced the dataset’s dimensionality from 3D to 2D.\n",
    "* Note that **the axes correspond to new features z1 and z2 (the coordinates of the projections on the plane).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* However, projection is not always the best approach to dimensionality reduction. \n",
    "* In many cases the subspace may twist and turn, such as in the famous Swiss roll toy dataset represented in Figure 8-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/08_06.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simply projecting onto a plane (e.g., by dropping x3) would squash different layers of the Swiss roll together, as shown on the left of Figure 8-5. \n",
    "* However, what you really want is to unroll the Swiss roll to obtain the 2D dataset on the right of Figure 8-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/08_07.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Manifold Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Swiss roll is an example of a 2D manifold. \n",
    "    * Put simply, a 2D manifold is a 2D shape that can be bent and twisted in a higher-dimensional space. \n",
    "    * More generally, a d-dimensional manifold is a part of an n-dimensional space (where d < n) that locally resembles a d-dimensional hyperplane. \n",
    "* In the case of the Swiss roll, d = 2 and n = 3: it locally resembles a 2D plane, but it is rolled in the third dimension.\n",
    "* Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie; this is called **Manifold Learning.** \n",
    "* It relies on the manifold assumption, also called the manifold hypothesis, which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold. \n",
    "* This assumption is very often empirically observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once again, think about the MNIST dataset: all handwritten digit images have some similarities. \n",
    "    * They are made of connected lines, the borders are white, they are more or less centered, and so on. \n",
    "    * If you randomly generated images, only a ridiculously tiny fraction of them would look like handwritten digits. \n",
    "    * In other words, <u>the degrees of freedom available to you if you try to create a digit image are dramatically lower than the degrees of freedom you would have if you were allowed to generate any image you wanted.</u> \n",
    "    * These constraints tend to squeeze the dataset into a lower-dimensional manifold.\n",
    "* The manifold assumption is often accompanied by another implicit assumption: that the task at hand (e.g., classification or regression) will be simpler if expressed in the lower-dimensional space of the manifold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/08_08.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For example, in the top row of Figure 8-6 the Swiss roll is split into two classes: in the 3D space (on the left), the decision boundary would be fairly complex, but in the 2D unrolled manifold space (on the right), the decision boundary is a simple straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* However, this assumption does not always hold. \n",
    "    * For example, in the bottom row of Figure 8-6, the decision boundary is located at x<sub>1</sub> = 5. \n",
    "    * This decision boundary looks very simple in the original 3D space (a vertical plane), but **it looks more complex in the unrolled manifold** (a collection of four independent line segments).\n",
    "* In short, if you reduce the dimensionality of your training set before training a model, it will definitely speed up training, but it may not always lead to a better or simpler solution; <u>it all depends on the dataset.</u>\n",
    "* Hopefully you now have a good sense of what the curse of dimensionality is and how dimensionality reduction algorithms can fight it, especially when the manifold assumption holds. \n",
    "* The rest of this chapter will go through some of the most popular algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Principal Component Analysis (PCA) is by far the most popular dimensionality reduction algorithm.\n",
    "* First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preserving the Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Before you can project the training set onto a lower-dimensional hyperplane, you first need to choose the right hyperplane. \n",
    "    * For example, a simple 2D dataset is represented on the left of Figure 8-7, along with three different axes (i.e., one-dimensional hyperplanes). \n",
    "    * On the right is the result of the projection of the dataset onto each of these axes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/08_09.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As you can see, the projection onto the solid line preserves the maximum variance, while the projection onto the dotted line preserves very little variance, and the projection onto the dashed line preserves an intermediate amount of variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It seems reasonable to select the axis that **preserves the maximum amount of variance**, as it will <u>most likely lose less information than the other projections</u>. \n",
    "* Another way to justify this choice is that it is the axis that <u>minimizes the mean squared distance between the original dataset and its projection onto that axis.</u>\n",
    "* This is the rather simple idea behind PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/08_09.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PCA identifies the axis that accounts for the largest amount of variance in the training set. In Figure 8-7, it is the **solid line.** \n",
    "* It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance. \n",
    "* In this 2D example there is no choice: it is the dotted line. \n",
    "* If it were a higher-dimensional dataset, PCA would also find a third axis, orthogonal to both previous axes, and a fourth, a fifth, and so on — as many axes as the number of dimensions in the dataset.\n",
    "* The unit vector that defines the i<sub>th</sub> axis is called the **i<sub>th</sub> principal component (PC).** \n",
    "* In Figure 8-7, the 1st PC is c<sub>1</sub> and the 2nd PC is c<sub>2</sub>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/08_04.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In Figure 8-2 the first two PCs are represented by the orthogonal arrows in the plane, and the third PC would be orthogonal to the plane (pointing up or down)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The direction of the principal components is not stable**: if you perturb the training set slightly and run PCA again, <u>some of the new PCs may point in the opposite direction of the original PCs.</u> However, they will generally still lie on the same axes.  \n",
    "In some cases, a pair of PCs may even rotate or swap, but the plane they define will generally remain the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So how can you find the principal components of a training set? Luckily, there is a standard matrix factorization technique called **Singular Value Decomposition (SVD)** that can decompose the training set matrix **X** into the dot product of three matrices **U · Σ · V<sup>T</sup>**, where **V<sup>T</sup>** contains all the principal components that we are looking for, as shown in Equation 8-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/08_10.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/08_15.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/08_16.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference : http://darkpgmr.tistory.com/106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The following Python code uses NumPy’s ***svd()*** function to obtain all the principal components of the training set, then extracts the first two PCs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = [[i**2, i, 1] for i in range(1930, 2020, 10)]\n",
    "matA = np.array(A).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.7249e+06, 1.9300e+03, 1.0000e+00],\n",
       "       [3.7636e+06, 1.9400e+03, 1.0000e+00],\n",
       "       [3.8025e+06, 1.9500e+03, 1.0000e+00],\n",
       "       [3.8416e+06, 1.9600e+03, 1.0000e+00],\n",
       "       [3.8809e+06, 1.9700e+03, 1.0000e+00],\n",
       "       [3.9204e+06, 1.9800e+03, 1.0000e+00],\n",
       "       [3.9601e+06, 1.9900e+03, 1.0000e+00],\n",
       "       [4.0000e+06, 2.0000e+03, 1.0000e+00],\n",
       "       [4.0401e+06, 2.0100e+03, 1.0000e+00]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = matA - matA.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.56666667e+05, -4.00000000e+01,  0.00000000e+00],\n",
       "       [-1.17966667e+05, -3.00000000e+01,  0.00000000e+00],\n",
       "       [-7.90666667e+04, -2.00000000e+01,  0.00000000e+00],\n",
       "       [-3.99666667e+04, -1.00000000e+01,  0.00000000e+00],\n",
       "       [-6.66666667e+02,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 3.88333333e+04,  1.00000000e+01,  0.00000000e+00],\n",
       "       [ 7.85333333e+04,  2.00000000e+01,  0.00000000e+00],\n",
       "       [ 1.18433333e+05,  3.00000000e+01,  0.00000000e+00],\n",
       "       [ 1.58533333e+05,  4.00000000e+01,  0.00000000e+00]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_centered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,s,V = np.linalg.svd(X_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5133311 ,  0.53477671, -0.03641862,  0.20799157,  0.34800135,\n",
       "         0.38361073,  0.31481971,  0.14162827, -0.13596357],\n",
       "       [-0.3865274 ,  0.13517892, -0.2513109 , -0.31529261, -0.29174947,\n",
       "        -0.18068149,  0.01791133,  0.30402898,  0.67767148],\n",
       "       [-0.25906838, -0.15046018,  0.93527962, -0.05342831, -0.03241249,\n",
       "        -0.00167292,  0.03879039,  0.08897745,  0.14888826],\n",
       "       [-0.13095404, -0.32214059, -0.09218599,  0.88245861, -0.10992061,\n",
       "        -0.06932365,  0.0042495 ,  0.11079883,  0.25032435],\n",
       "       [-0.00218439, -0.37986231, -0.09203969, -0.13973421,  0.85582405,\n",
       "        -0.1053649 , -0.02330106,  0.10201555,  0.27058495],\n",
       "       [ 0.12724058, -0.32362533, -0.06428148, -0.12000677, -0.1351785 ,\n",
       "         0.89020332, -0.04386131,  0.0626276 ,  0.20967007],\n",
       "       [ 0.25732087, -0.15342966, -0.00891137, -0.05835906, -0.08292827,\n",
       "        -0.08261899,  0.94256876, -0.00736501,  0.0675797 ],\n",
       "       [ 0.38805647,  0.1307247 ,  0.07407065,  0.04520891,  0.01257475,\n",
       "        -0.02383184, -0.06401085,  0.89203771, -0.15568615],\n",
       "       [ 0.51944738,  0.52883775,  0.18466458,  0.19069715,  0.15133055,\n",
       "         0.06656479, -0.06360014, -0.23916423,  0.53987251]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.05196143e+05, 4.45422285e-01, 0.00000000e+00])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eigenvalue of matA\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.99999968e-01,  2.53798706e-04,  0.00000000e+00],\n",
       "       [ 2.53798706e-04, -9.99999968e-01, -0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = V.T[:,0]\n",
    "c2 = V.T[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.99999968e-01, 2.53798706e-04, 0.00000000e+00])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.53798706e-04, -9.99999968e-01, -0.00000000e+00])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **WARNING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA assumes that **the dataset is centered around the origin.** As we will see, Scikit-Learn’s PCA classes take care of centering the data for you. However, if you implement PCA yourself (as in the preceding example), or if you use other libraries, don’t forget to center the data first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Projecting Down to d Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once you have identified all the principal components, you can <u>reduce the dimensionality of the dataset</u> down to ***d* dimensions by projecting it onto the hyperplane defined by the first *d* principal components.** \n",
    "* Selecting this hyperplane ensures that the projection will preserve as much variance as possible. \n",
    "    * For example, in Figure 8-2 the 3D dataset is projected down to the 2D plane defined by the first two principal components, preserving a large part of the dataset’s variance. \n",
    "    * As a result, the 2D projection looks very much like the original 3D dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/08_04.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To project the training set onto the hyperplane, you can simply compute the dot product of the training set matrix **X** by the matrix **W<sub>d</sub>**, defined as the matrix containing the first d principal components (i.e., the matrix composed of the first *d* columns of **V<sup>T</sup>**), as shown in Equation 8-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/08_11.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python code projects the training set onto the plane defined by the first two principal\n",
    "components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = V.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.56666672e+05,  2.38201465e-01],\n",
       "       [-1.17966670e+05,  6.02117031e-02],\n",
       "       [-7.90666692e+04, -6.70183182e-02],\n",
       "       [-3.99666679e+04, -1.43488598e-01],\n",
       "       [-6.66666645e+02, -1.69199137e-01],\n",
       "       [ 3.88333346e+04, -1.44149935e-01],\n",
       "       [ 7.85333359e+04, -6.83409916e-02],\n",
       "       [ 1.18433337e+05,  5.82276930e-02],\n",
       "       [ 1.58533338e+05,  2.35556119e-01]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it! You now know how to reduce the dimensionality of any dataset down to any number of dimensions, while preserving as much variance as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scikit-Learn’s *PCA* class implements PCA using SVD decomposition just like we did before. \n",
    "* The following code applies PCA to reduce the dimensionality of the dataset down to two dimensions (note that it automatically takes care of centering the data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(matA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.7249e+06, 1.9300e+03, 1.0000e+00],\n",
       "       [3.7636e+06, 1.9400e+03, 1.0000e+00],\n",
       "       [3.8025e+06, 1.9500e+03, 1.0000e+00],\n",
       "       [3.8416e+06, 1.9600e+03, 1.0000e+00],\n",
       "       [3.8809e+06, 1.9700e+03, 1.0000e+00],\n",
       "       [3.9204e+06, 1.9800e+03, 1.0000e+00],\n",
       "       [3.9601e+06, 1.9900e+03, 1.0000e+00],\n",
       "       [4.0000e+06, 2.0000e+03, 1.0000e+00],\n",
       "       [4.0401e+06, 2.0100e+03, 1.0000e+00]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.56666672e+05,  2.38201465e-01],\n",
       "       [-1.17966670e+05,  6.02117031e-02],\n",
       "       [-7.90666692e+04, -6.70183182e-02],\n",
       "       [-3.99666679e+04, -1.43488598e-01],\n",
       "       [-6.66666645e+02, -1.69199137e-01],\n",
       "       [ 3.88333346e+04, -1.44149935e-01],\n",
       "       [ 7.85333359e+04, -6.83409916e-02],\n",
       "       [ 1.18433337e+05,  5.82276930e-02],\n",
       "       [ 1.58533338e+05,  2.35556119e-01]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After fitting the PCA transformer to the dataset, you can access the principal components using the *components_* variable (note that it contains the PCs as horizontal vectors, so, for example, the first principal component is equal to *pca.components_.T[:, 0]*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Explained Variance Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Another very useful piece of information is the explained variance ratio of each principal component, available via the *explained_variance_ratio_ variable.* \n",
    "* It indicates the proportion of the dataset’s variance that lies along the axis of each principal component. \n",
    "* For example, let’s look at the explained variance ratios of the first two components of the 3D dataset represented in Figure 8-2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/08_04.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1300303957016196e-12\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This tells you that 100% of the dataset’s variance lies along the first axis, and 0.000...2% lies along the second axis. \n",
    "* This leaves less than 1.2% for the third axis, so it is reasonable to assume that it probably carries little information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Choosing the Right Number of Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instead of arbitrarily choosing the number of dimensions to reduce down to, it is generally preferable to **choose the number of dimensions that add up to a sufficiently large portion of the variance** (e.g., 95%). \n",
    "* Unless, of course, you are reducing dimensionality for data visualization — in that case you will generally want to reduce the dimensionality down to **2 or 3.**\n",
    "* The following code <u>computes PCA without reducing dimensionality,</u> then **computes the minimum number of dimensions required to preserve 95% of the training set’s variance:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca= PCA()\n",
    "pca.fit(matA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**numpy.cumsum**  \n",
    "Return the cumulative sum of the elements along a given axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0000000e+00, 2.1300304e-12, 0.0000000e+00])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(np.argmax(cumsum >= 0.95))\n",
    "d = np.argmax(cumsum >= 0.95) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You could then set *n_components=d* and run PCA again. \n",
    "* However, there is a much better option:\n",
    "    * instead of specifying the number of principal components you want to preserve, you can set *n_components* to be a float between **0.0 and 1.0**, indicating the ratio of variance you wish to preserve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=d)\n",
    "X_reduced = pca.fit_transform(matA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-156666.67177287],\n",
       "       [-117966.67048129],\n",
       "       [ -79066.66919615],\n",
       "       [ -39966.66791745],\n",
       "       [   -666.6666452 ],\n",
       "       [  38833.33462062],\n",
       "       [  78533.33587999],\n",
       "       [ 118433.33713293],\n",
       "       [ 158533.33837942]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(matA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-156666.67177287],\n",
       "       [-117966.67048129],\n",
       "       [ -79066.66919615],\n",
       "       [ -39966.66791745],\n",
       "       [   -666.6666452 ],\n",
       "       [  38833.33462062],\n",
       "       [  78533.33587999],\n",
       "       [ 118433.33713293],\n",
       "       [ 158533.33837942]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/08_12.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Yet another option is to plot the explained variance as a function of the number of dimensions (simply plot cumsum; see Figure 8-8). \n",
    "* There will usually be an elbow in the curve, where the explained variance stops growing fast. \n",
    "* You can think of this as the intrinsic dimensionality of the dataset. In this case, you can see that reducing the dimensionality down to about 100 dimensions wouldn’t lose too much explained variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### PCA for Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obviously after dimensionality reduction, the training set takes up much less space. \n",
    "    * For example, try applying PCA to the MNIST dataset while preserving 95% of its variance. \n",
    "    * You should find that each instance will have just over 150 features, instead of the original 784 features. \n",
    "* So <u>while most of the variance is preserved, the dataset is now less than 20% of its original size!</u> \n",
    "* This is a reasonable compression ratio, and you can see how this can speed up a classification algorithm (such as an SVM classifier) tremendously.\n",
    "* It is also possible to decompress the reduced dataset **back to 784 dimensions by applying the inverse transformation of the PCA projection.** \n",
    "* Of course this won’t give you back the original data, since the projection lost a bit of information (within the 5% variance that was dropped), <u>but it will likely be quite close to the original data.</u> \n",
    "* The mean squared distance between the original data and the reconstructed data (compressed and then decompressed) is called **the reconstruction error.**\n",
    "    * For example, the following code compresses the MNIST dataset down to 154 dimensions, then uses the *inverse_transform()* method to decompress it back to 784 dimensions. \n",
    "    * Figure 8-9 shows a few digits from the original training set (on the left), and the corresponding digits after compression and decompression. \n",
    "    * You can see that there is a slight image quality loss, **but the digits are still mostly intact.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y_mnist), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mnist = X.reshape(-1, 28*28)\n",
    "X_mnist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=154)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mnist_reduces = pca.fit_transform(X_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.23932589e+02, -3.12674262e+02, -2.45140519e+01, ...,\n",
       "        -2.90325797e+01,  1.48880978e+01,  2.78623778e+01],\n",
       "       [ 1.01171838e+03, -2.94857038e+02,  5.96339561e+02, ...,\n",
       "        -2.94593548e+01,  1.16299590e+01,  2.96166390e+01],\n",
       "       [-5.18496081e+01,  3.92173153e+02, -1.88509749e+02, ...,\n",
       "         3.85633109e+01, -7.45990727e+01, -1.03204927e+02],\n",
       "       ...,\n",
       "       [-1.78053450e+02,  1.60078211e+02, -2.57613082e+02, ...,\n",
       "        -1.10195242e-01, -2.43054643e+01, -1.16322675e+01],\n",
       "       [ 1.30606072e+02, -5.59193641e+00,  5.13858674e+02, ...,\n",
       "         3.61727474e+01, -5.32494857e+01,  5.26154293e+01],\n",
       "       [-1.73435952e+02, -2.47188023e+01,  5.56018894e+02, ...,\n",
       "        -1.71339258e+01, -2.26387708e+01, -6.75121186e+00]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mnist_reduces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 154)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mnist_reduces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mnist_recovered = pca.inverse_transform(X_mnist_reduces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mnist_recovered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X_mnist_recovered[0].reshape(1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23004abd828>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 9))\n",
    "ax1.imshow(X[0],cmap='Greys_r') \n",
    "ax2.imshow(test[0],cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/08_13.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation of the inverse transformation is shown in Equation 8-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/08_14.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Incremental PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One problem with the preceding implementation of PCA is that it requires the whole training set to fit in memory in order for the SVD algorithm to run. \n",
    "* Fortunately, **Incremental PCA (IPCA)** algorithms have been developed: you can split the training set into mini-batches and feed an IPCA algorithm one mini-batch at a time. \n",
    "* This is useful for large training sets, and also to apply PCA online (i.e., on the fly, as new instances arrive).\n",
    "* The following code splits the MNIST dataset into 100 mini-batches (using NumPy’s *array_split()* function) and feeds them to Scikit-Learn’s *IncrementalPCA* class to reduce the dimensionality of the MNIST dataset down to 154 dimensions (just like before). \n",
    "* Note that you must call the *partial_fit()* method with each mini-batch rather than the *fit()* method with the whole training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches=100\n",
    "inc_pca = IncrementalPCA(n_components=154)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_batch in np.array_split(X_mnist, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mnist_reduced = inc_pca.transform(X_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 154)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mnist_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Alternatively, you can use NumPy’s *memmap* class, which allows you to manipulate a large array stored in a binary file on disk as if it were entirely in memory; the class loads only the data it needs in memory, when it needs it. \n",
    "* Since the *IncrementalPCA* class uses only a small part of the array at any given time, the memory usage remains under control. \n",
    "* This makes it possible to call the usual *fit()* method, as you can see in the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**np.memmap**  \n",
    "Create a memory-map to an array stored in a binary file on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bin = X_mnist.tobytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47040000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 60000\n",
    "n = 786"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x88 in position 157: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-ce4094e43f6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"readonly\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\memmap.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(subtype, filename, dtype, mode, offset, shape, order)\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mf_ctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontextlib_nullcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[0mf_ctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'r'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mf_ctx\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x88 in position 157: invalid start byte"
     ]
    }
   ],
   "source": [
    "X_mm = np.memmap(X_bin, dtype=\"float32\", mode=\"readonly\",shape=(m,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = m//n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Randomized PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scikit-Learn offers yet another option to perform PCA, called *Randomized PCA*. \n",
    "* This is a stochastic algorithm that quickly finds an approximation of the first *d* principal components. \n",
    "* Its computational complexity is *O(m × d<sup>2</sup>) + O(d<sup>3</sup>)*, instead of *O(m × n<sup>2</sup>) + O(n<sup>3</sup>)*, so it is dramatically faster than the previous algorithms when *d* is much smaller than *n*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=154)\n",
    "X_mnist_reduces = pca.fit_transform(X_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 154)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mnist_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 154)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
